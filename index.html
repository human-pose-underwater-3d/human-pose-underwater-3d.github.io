<!DOCTYPE html>
<html>
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-61302010-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-61302010-1');
    </script>
    <title>Stereo-Based 3D Human Pose Estimation for Underwater Divers Without 3D Supervision</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <!-- <link rel='stylesheet' href='https://fonts.googleapis.com/css?family=Roboto'> -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <style>
      html,body,h1,h2,h3,h4,h5,h6 {font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;}
      <!-- .cite { background:#f0f0f0; padding:10px; font-size:18px} -->
      .cite { padding:0px; background:#ffffff; font-size:18px}
      .card {border: 1px solid #ccc}
      img { margin-bottom:-6px;}
      p { font-size:18px;}
      a {text-decoration: none; color: #2196F3;}
      .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      box-shadow:
      0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
      5px 5px 0 0px #fff, /* The second layer */
      5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
      10px 10px 0 0px #fff, /* The third layer */
      10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
      15px 15px 0 0px #fff, /* The fourth layer */
      15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
      20px 20px 0 0px #fff, /* The fifth layer */
      20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
      25px 25px 0 0px #fff, /* The fifth layer */
      25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
      margin-left: 10px;
      margin-right: 60px;
      }
    </style>
    <meta name="google-site-verification" content="8Q_ytX8FqHWcIDBc2IoJAwkJ35JRHclQw494GYdlHBE" />
  </head>  
  <body class="w3-white">
    <!-- Page Container -->
    <div class="w3-content w3-margin-top w3-margin-bottom" style="max-width:960px;">

      <!-- The Grid -->
      <div class="w3-row-padding">

	<!-- paper container -->	  
	<div class="w3-display-container w3-row w3-white w3-margin-bottom">
	  <div class="w3-center">
	    <h1>Stereo-Based 3D Human Pose Estimation for Underwater Divers Without 3D Supervision</h1>
	    <h5><a href="">Ying-Kun Wu</a> &emsp;&emsp; <a href="https://junaedsattar.cs.umn.edu/">Junaed Sattar</a></h5>
	    <h5><em>RA-L</em></h5>
	  </div>
	  <div class="w3-center">
	    <!-- <h3>[<a href="https://github.com/aharley/pips">Code</a>] &emsp; [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Harley_Track_Check_Repeat_An_EM_Approach_to_Unsupervised_Tracking_CVPR_2021_paper.pdf">Paper</a>]</h3> -->
	    <h3>[<a href="">Paper (reviewing)</a>] &emsp; [<a href="https://github.com/yingkunwu/poseiden">Code</a>]</h3>
	  </div>
	  
	  <hr>
	  <div class="w3-center">
	    <h2>Abstract</h2>
	  </div>
	  <p>In this paper, we propose a novel deep learning-based 3D underwater human pose estimator capable of providing absolute-scale 3D poses of scuba divers from stereo image pairs. While existing research has made significant advancements in 3D human pose estimation, most methods rely on 3D ground truth for training. To overcome this, our approach leverages epipolar geometry to derive 3D information from 2D estimations. Our method estimates 2D human poses while capturing their corresponding disparity from binocular perspectives, thus avoiding challenges in finding per-pixel correspondences in textureless regions commonly seen underwater. Additionally, to reduce the sensitivity of our method on 2D annotation accuracy, we design an auto-refinement pipeline to automatically correct biases introduced by human labeling. Experiments demonstrate that our approach significantly improves performance compared to previous state-of-the-art methods in dynamic environments while being efficient enough to run on limited-capacity edge devices.</p>
	  <hr>

    <div class="w3-center"></div>
	    <h2>DiverPose Dataset</h2>
	  </div>
	  <p>To train our model, we collected our train dataset that includes footage from both closed-water and open-water environments. It also features challenging poses that divers can perform easily underwater, such as swimming upside down.</p>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:10px;">
	    <tr style="padding:0px">
	      <td style="padding-right:1%;width:32%;vertical-align:middle">

		<video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/zed_2024-02-12-18-35-23.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		</video>
		
	      </td>
	      <td style="padding-right:1%;width:32%;vertical-align:middle">

		<video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/zed_2024-02-12-18-41-20.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		</video>
		
	      </td>
	    </tr>
	  </table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:10px;">
	    <tr style="padding:0px">
	      <td style="padding-right:1%;width:32%;vertical-align:middle">

		<video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/camel_mask.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		</video>
		
	      </td>
	      <td style="padding-right:1%;width:32%;vertical-align:middle">

		<video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/dancer_mask.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		</video>
		
	      </td>
	    </tr>
	  </table>
    <hr>

	  <div class="w3-center">
	    <h2>Demo</h2>
	  </div>
	  <p><strong>Goal</strong>: Given a target pixel specified on the first frame, track that pixel across the whole video.</p>
	  <!-- <p>Unlike feature-matching or flow-based methods, particle trajectories can track objects during and through occlusions.</p> -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
	    <tr style="padding:0px">
	      <td style="padding-right:1%;width:32%;vertical-align:middle">
		<div align="center">DINO</div>
	      </td>
	      <td style="padding-right:1%;width:32%;vertical-align:middle">
		<div align="center">RAFT</div>
	      </td>
	      <td style="padding-right:1%;width:32%;vertical-align:middle">
		<div align="center">PIPs (ours)</div>
	      </td>
	    </tr>
	  </table>
	  <table>
	    <tr style="padding:0px">
	      <td style="width:100%;vertical-align:middle">
		<video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/horse_all.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		</video>
	      </td>
	    </tr>
	  </table>
	  <table>
	    <tr style="padding:0px">
	      <td style="width:100%;vertical-align:middle">
		<video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/camel_all.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		</video>
	      </td>
	    </tr>
	  </table>
	  <table>
	    <tr style="padding:0px">
	      <td style="width:100%;vertical-align:middle">
		<video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/dog_all.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		</video>
	      </td>
	    </tr>
	  </table>
	  <table>
	    <tr style="padding:0px">
	      <td style="width:100%;vertical-align:middle">
		<video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/horse2_all.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		</video>
	      </td>
	    </tr>
	  </table>

	  <!-- <p>Our model is trained once, on a large synthetic dataset.</p>n  -->
	  <p>We call our method <strong>Persistent Independent Particles</strong>, because we treat each pixel as if it is a particle with a long-range trajectory, and we track each particle <em>independently</em>. Some computation is shared between particles, which makes inference fast, but each particle produces its own trajectory, without inspecting the trajectories of its neighbors. A side-effect of this is that we may compute particle trajectories for any given subset of pixels. For the visualization below, we computed particles at three grids of different densities.</p>
	  <table>
	    <tr style="padding:0px">
	      <td style="width:100%;vertical-align:middle">
		<video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/pup_vstack.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		</video>
	      </td>
	    </tr>
	  </table>

	  
	  <p>Our method initializes a zero-velocity trajectory (copying the initial position across time), and then iteratively refines the trajectory, using local appearance costs, and a learned trajectory prior. Since it reasons over multiple timesteps simultaneously, it "catch" a target after it re-emerges from an occluder, and inpaint the missing part of the trajectory.</p>
	  <table>
	    <tr style="padding:0px">
	      <td style="width:100%;vertical-align:middle">
		<!-- <img src="images/iterative_updates2.gif" style="width:100%"> -->
		<video controls style="width:100%;max-width:100%" autoplay="true" loop="true" playsinline="true" muted="true">
		  <source src="videos/iterative_updates2.mp4" type="video/mp4">Sorry, your browser doesn't support embedded videos.
		</video>
		
	      </td>
	    </tr>
	  </table>

	  
	  <!-- <div class="w3-display-container w3-row w3-white w3-margin-bottom w3-center"> -->
	  <!--   <iframe width="800" height="600" src="https://www.youtube.com/embed/Jg2f5fkgxZo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
	  <!-- </div> -->
	  <hr>
	  

	  <div class="w3-row w3-margin" style="padding-bottom:2em">
	    <div class="w3-center"><h2>Paper</h2></div>
	    <div class="w3-col s0 m1 l2" style="height:10px"></div>
	    <div class="w3-col s6 m3 l2">
	      <a href="https://arxiv.org/abs/2204.04153"><img class="layered-paper-big" src="images/page.png" style="width:100%;min-height:200px; margin-right:3em"></a>
	    </div>
	    <div class="w3-col s6 m7 l6" style="padding-left:5em">
	      <div class="cite">
		Adam W. Harley, Zhaoyuan Fang, and Katerina Fragkiadaki.
		<i>Particle Video Revisited: Tracking Through Occlusions Using Point Trajectories.</i> 
		ECCV 2022.
	      </div>
	      <h3><a href="https://arxiv.org/pdf/2204.04153.pdf">[pdf]</a>&emsp;<a href="bib.txt">[bibtex]</a></h3>
	    </div>
	    <div class="w3-col s0 m1 l2" style="height:10px"></div>
	  </div>
	  <hr>
	  
	  <!-- end paper container -->

	</div><!-- End Grid -->
      </div><!-- End Page Container -->

  </body>
</html>